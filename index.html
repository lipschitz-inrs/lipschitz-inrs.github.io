<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Beyond Uniformity: Regularizing Implicit Neural Representations through a Lipschitz Lens">
    <meta name="keywords"
          content="Implicit Neural Representations, Neural Fields, Lipschitz, Regularization, INR, Deep Learning, Medical Imaging, Registration, Inpainting">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Beyond Uniformity: Regularizing INRs through a Lipschitz Lens</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ extensions: ["tex2jax.js"], jax: ["input/TeX", "output/HTML-CSS"], tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], displayMath: [ ['$$','$$'], ["\\[","\\]"] ], processEscapes: true }, "HTML-CSS": { fonts: ["TeX"] } });
    </script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        .method-image {
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .result-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin-top: 1.5rem;
        }
        .highlight-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1rem 0;
        }
        .contribution-item {
            background: #f8f9fa;
            padding: 1rem;
            border-radius: 8px;
            margin-bottom: 0.75rem;
            border-left: 4px solid #667eea;
        }
        .equation-box {
            background: #f5f5f5;
            padding: 1rem;
            border-radius: 8px;
            margin: 1rem 0;
            text-align: center;
        }
        .example-box {
            background: #e8f4f8;
            border-left: 4px solid #00a0b0;
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 0 8px 8px 0;
        }
    </style>
</head>

<body>
<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="https://jqmcginnis.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
            </a>
        </div>
    </div>
</nav>

<!-- Hero. -->
<section class="hero">
  <div class="hero-body" style="padding-bottom: 1rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Beyond Uniformity: Regularizing Implicit Neural Representations through a Lipschitz Lens</h1>
          <h4 class="title is-4">ICLR 2026</h4>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jqmcginnis.github.io">Julian McGinnis</a><sup><small>1,2,*</small></sup>,
            </span>
            <span class="author-block">
              <a href="#">Suprosanna Shit</a><sup><small>3,4,*</small></sup>,
            </span>
            <span class="author-block">
              <a href="#">Florian A. Hölzl</a><sup><small>1,5</small></sup>,
            </span>
            <span class="author-block">
              <a href="https://pfriedri.github.io">Paul Friedrich</a><sup><small>6</small></sup>,
            </span>
            <span class="author-block">
              <a href="#">Paul Büschl</a><sup><small>3</small></sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="#">Vasiliki Sideri-Lampretsa</a><sup><small>1</small></sup>,
            </span>
            <span class="author-block">
              <a href="#">Mark Mühlau</a><sup><small>1</small></sup>,
            </span>
            <span class="author-block">
              <a href="#">Philippe C. Cattin</a><sup><small>6</small></sup>,
            </span>
            <span class="author-block">
              <a href="#">Björn Menze</a><sup><small>3</small></sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://www.professoren.tum.de/en/rueckert-daniel">Daniel Rueckert</a><sup><small>1,2,7,†</small></sup>,
            </span>
            <span class="author-block">
              <a href="#">Benedikt Wiestler</a><sup><small>1,2,†</small></sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">
            <span class="author-block"><sup><small>*</small></sup>Equal contribution</span>
            <span class="author-block"><sup><small>†</small></sup>Equal senior contribution</span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top: 0.5rem;">
            <span class="author-block"><sup><small>1</small></sup>Technical University of Munich</span>
            <span class="author-block"><sup><small>2</small></sup>MCML</span>
            <span class="author-block"><sup><small>3</small></sup>University of Zurich</span>
            <br>
            <span class="author-block"><sup><small>4</small></sup>ETH AI Centre</span>
            <span class="author-block"><sup><small>5</small></sup>Hasso-Plattner-Institute</span>
            <span class="author-block"><sup><small>6</small></sup>University of Basel</span>
            <span class="author-block"><sup><small>7</small></sup>Imperial College London</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
            <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
                </a>
            </span>

            <span class="link-block">
                <a href="" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
                </a>
            </span>

            <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
                </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Eyecatcher. -->
<section class="eyecatcher" style="padding-top: 0;">
    <div class="container is-max-desktop">
        <div class="hero-body" style="padding-top: 1rem;">
            <div class="has-text-centered">
                <img src="images/figure1_teaser.png" width="70%" class="method-image">
            </div>
            <h1 class="subtitle has-text-justified is-size-6" style="margin-top: 1rem;">
                <b>Bridging the gap between theory and practice in Lipschitz regularization.</b> While Lipschitz continuity offers a principled form of implicit regularization, <i>selecting</i> and <i>distributing</i> the budget K remains non-trivial. We propose a novel, data-driven approach to derive K from interpretable, signal- or domain-specific properties, such as tissue compressibility in deformable registration, and strategically distribute this budget across layers. This methodology allows to better balance smoothness and expressiveness compared to uniform allocation strategies.
            </h1>
        </div>
    </div>
</section>

<!-- Abstract. -->
<section class="section" style="background-color:#f5f5f5; padding-bottom: 2rem;">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Implicit Neural Representations (INRs) have shown great promise in solving inverse problems, but their lack of inherent regularization often leads to a trade-off between expressiveness and smoothness. While Lipschitz continuity presents a principled form of implicit regularization, it is often applied as a rigid, uniform 1-Lipschitz constraint, limiting its potential in inverse problems.
                    </p>
                    <p>
                        In this work, we reframe Lipschitz regularization as a flexible <b>Lipschitz budget framework</b>. We propose a method to first derive a principled, task-specific total budget K, then proceed to distribute this budget <i>non-uniformly</i> across all network components, including linear weights, activations, and embeddings.
                    </p>
                    <p>
                        Across extensive experiments on deformable registration and image inpainting, we show that non-uniform allocation strategies provide a measure to balance regularization and expressiveness within the specified global budget. Our <i>Lipschitz lens</i> introduces an alternative, interpretable perspective to Neural Tangent Kernel (NTK) and Fourier analysis frameworks in INRs, offering practitioners actionable principles for improving network architecture and performance.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Method Overview -->
<section class="section" style="background-color:#f5f5f5; padding-top: 2rem;">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Method Overview</h2>
        
        <div class="content has-text-justified">
            <h3 class="title is-4">Layer-wise Lipschitz Composition</h3>
            <p>
                For a network \(f_\theta\) with L layers, the overall Lipschitz constant K is bounded by the product of individual layer and activation Lipschitz constants:
            </p>
            <div class="equation-box">
                $$ K = \text{Lip}(f_\theta) \leq \prod_{i=1}^{L} \text{Lip}(\phi_i) \cdot \text{Lip}(\mathbf{W}_i) $$
            </div>
            <p>
                This compositional property reveals that a global budget K can be achieved through various combinations of layer-wise constants, motivating our exploration of different budget allocation strategies.
            </p>
        </div>

        <div class="content has-text-justified" style="margin-top: 2rem;">
            <h3 class="title is-4">Budget Allocation Strategies</h3>
            <p>Given the total Lipschitz budget \(K_B\), we investigate multiple allocation strategies:</p>
            <ul>
                <li><b>(A) Uniform:</b> Each component receives equal contribution: \(K_i = \sqrt[M]{K_B}\)</li>
                <li><b>(B) All-first:</b> Allocate \(K_1 = K_B\) and \(K_i = 1\) for remaining layers</li>
                <li><b>(C) Linear:</b> Monotonically increasing allocation from first to last layer</li>
                <li><b>(D) Exponential:</b> Front-heavy ramp in log space</li>
                <li><b>(E) Cosine-annealed:</b> Smooth transition using cosine schedule</li>
            </ul>
        </div>
    </div>
</section>

<!-- Shape Results -->
<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Results: 1-Lipschitz Signed Distance Fields</h2>
        
        <div class="content has-text-justified">
            <p>
                We investigate how different spectral normalization techniques and gradient-preserving activation functions affect SDF learning quality. Our key finding: <b>approaching the upper Lipschitz bound correlates with better perceptual quality</b>.
            </p>
        </div>

        <div class="has-text-centered" style="margin-top: 1.5rem;">
            <img src="images/figure2_bunny.png" width="75%" class="method-image">
            <p class="is-size-6 has-text-justified" style="margin-top: 1rem;">
                <b>Learning the Stanford bunny</b> with different spectral normalization techniques (first row) and different gradient-preserving activation functions with Bjorck normalized layers (second row). We report Chamfer Distance (CD↓) and empirically estimated Lipschitz constant \(K_m\). Networks that approach the 1-Lipschitz budget more closely achieve better reconstruction quality.
            </p>
        </div>

        <div class="has-text-centered" style="margin-top: 2rem;">
            <img src="images/figure3_allocation_sdf.png" width="100%" class="method-image">
            <p class="is-size-6 has-text-justified" style="margin-top: 1rem;">
                <b>Budget allocation experiments for 1-Lipschitz SDFs.</b> Training with Householder (HH) and GroupSort (MaxMin) activations using Bjorck and SLL normalization shows that non-uniform allocation strategies perform on par with uniform approaches—suggesting the benefits may be stifled by the overly restrictive unit budget.
            </p>
        </div>
    </div>
</section>

<!-- Registration Results -->
<section class="section" style="background-color:#f5f5f5">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Results: Deformable Image Registration</h2>
        
        <div class="content has-text-justified">
            <p>
                For lung CT registration, we derive the Lipschitz budget from clinical evidence: strain approaching 2.0 marks a threshold for tissue failure. This motivates our choice of \(K = 2\) as a principled, domain-driven budget.
            </p>
        </div>

        <div class="has-text-centered" style="margin-top: 1.5rem;">
            <img src="images/figure4_registration.png" width="80%" class="method-image">
            <p class="is-size-6 has-text-justified" style="margin-top: 1rem;">
                <b>Comparison of smoothness (Folding Ratio↓) and expressiveness (TRE↓)</b> across Lipschitz-regularized INR architectures. Non-uniform allocations (e.g., exponential) can improve target registration error while maintaining comparable folding ratio to uniform allocation.
            </p>
        </div>

    </div>
</section>

<!-- Inpainting Results -->
<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Results: Image Inpainting</h2>
        
        <div class="content has-text-justified">
            <p>
                For image inpainting, we derive the Lipschitz budget using a <b>data-driven oracle</b> based on the maximum spectral norm of the image Jacobian. This provides an interpretable upper bound on expected signal variation.
            </p>
        </div>

        <div class="has-text-centered" style="margin-top: 1.5rem;">
            <img src="images/figure5_inpainting_quant.png" width="80%" class="method-image">
            <p class="is-size-6 has-text-justified" style="margin-top: 1rem;">
                <b>Quantitative results for different allocation strategies</b> in inpainting with FFNs. Performance peaks near the oracle estimate and degrades when the Lipschitz budget deviates, demonstrating the oracle provides a meaningful approximation.
            </p>
        </div>

        <div class="has-text-centered" style="margin-top: 2rem;">
            <img src="images/figure6_inpainting_qual.png" width="80%" class="method-image">
            <p class="is-size-6 has-text-justified" style="margin-top: 1rem;">
                <b>Qualitative examples from CelebA inpainting.</b> Non-uniform budget allocation strategies (First, Linear, Exponential) yield statistically significant improvements over uniform allocation, particularly near the oracle estimate (Distance=0).
            </p>
        </div>
    </div>
</section>

<!-- Lipschitz Perspective -->
<section class="section" style="background-color:#f5f5f5">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">A Novel Perspective: Weight Scaling through a Lipschitz Lens</h2>
        
        <div class="content has-text-justified">
            <p>
                Recent work shows that scaling SIREN weights by a factor \(\alpha\) improves accuracy and convergence. While existing explanations rely on NTK theory, we show that <b>Lipschitz theory offers a complementary perspective</b>: for a SIREN layer \(f(x) = \sin(\omega(\alpha \mathbf{W}x + b))\), the induced Lipschitz constant is \(\text{Lip}(f) = |\omega\alpha\|\mathbf{W}\||\), scaling linearly with \(\alpha\).
            </p>
        </div>

        <div class="has-text-centered" style="margin-top: 1.5rem;">
            <img src="images/figure7_weight_scaling.png" width="100%" class="method-image">
            <p class="is-size-6 has-text-justified" style="margin-top: 1rem;">
                <b>Visualization of the upper induced Lipschitz bound</b> for weight scaling in SIREN. Scaling the initialization leads to direct scaling of layer Lipschitz bounds, allowing the network to increase its capacity for high-frequency content. Layer-wise analysis shows self-regulating capacity when weights are sufficiently scaled.
            </p>
        </div>
    </div>
</section>

<!-- Practical Guidelines -->
<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered">Practical Guidelines</h2>
        
        <div class="columns">
            <div class="column">
                <h4 class="title is-5">Estimating Budget K</h4>
                <div class="content">
                    <ul>
                        <li><b>Domain-driven:</b> Use physical constraints (e.g., max tissue strain ≈2.0 for lung registration, max cardiac contraction) or intensity bounds as interpretable upper bounds.</li>
                        <li><b>Data-driven:</b> Compute an oracle from representative high-resolution samples based on the maximum spectral norm of the image Jacobian. See the paper for the full derivation.</li>
                        <li><b>Signal-theoretic:</b> Use known bandlimits or sampling rates (e.g., 44.1 kHz for audio, ≈150 Hz for ECG) as conservative, noise-suppressing estimates.</li>
                    </ul>
                </div>
                
                <div class="example-box">
                    <b>Tip:</b> For noisy images, use percentile-based estimates (99.9%) rather than the absolute maximum to improve robustness.
                </div>
            </div>
            <div class="column">
                <h4 class="title is-5">Allocating Budget K</h4>
                <div class="content">
                    <ul>
                        <li>Treat allocation as a hyperparameter search centered on network expressivity</li>
                        <li>Analyze performance with respect to \(K_{min}\), the minimum imposed Lipschitz bound</li>
                        <li>Non-uniform strategies enable balancing regularization with expressiveness under the same global budget</li>
                    </ul>
                </div>
            </div>
        </div>
        
        <div class="content has-text-justified" style="margin-top: 1rem;">
            <h4 class="title is-5">Key Insights</h4>
            <ul>
                <li>For 1-Lipschitz SDFs, non-uniform allocation performs on par with uniform—benefits emerge in general K-Lipschitz settings where the budget itself is task-dependent.</li>
                <li>Non-uniform strategies (e.g., exponential) can improve TRE in registration while maintaining comparable folding ratio.</li>
                <li>Performance peaks near the oracle estimate and degrades when the budget deviates significantly.</li>
                <li>Different architectures exhibit varying degrees of self-regulation: SIREN shows superior self-regulation compared to FFNs.</li>
            </ul>
        </div>
    </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX_Sec" style="background-color:#f5f5f5">
    <div class="container is-max-desktop content" style="position: relative;">
        <h2 class="title">BibTeX</h2>

        <div style="position: relative; display: inline-block; width: 100%;">
            <button onclick="copyBibTeX()" class="button is-small is-primary"
                    style="position: absolute; top: 10px; right: 10px; z-index: 10;">
                Copy
            </button>
            <pre style="position: relative; padding-right: 5px;">
      <code id="bibtex">
@inproceedings{mcginnis2026beyond,
    title={Beyond Uniformity: Regularizing Implicit Neural Representations through a Lipschitz Lens},
    author={Julian McGinnis and Suprosanna Shit and Florian A. H{\"o}lzl and Paul Friedrich and Paul B{\"u}schl and Vasiliki Sideri-Lampretsa and Mark M{\"u}hlau and Philippe C. Cattin and Bj{\"o}rn Menze and Daniel Rueckert and Benedikt Wiestler},
    booktitle={International Conference on Learning Representations},
    year={2026},
    url={https://openreview.net/forum?id=XXXXX}
}</code>
      </pre>
        </div>
    </div>
</section>

<script>
    function copyBibTeX() {
        var bibtex = document.getElementById("bibtex").innerText;
        navigator.clipboard.writeText(bibtex).catch(err => {
            console.error("Failed to copy: ", err);
        });
    }
</script>


<section class="section" id="Acknowledgements">
    <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgements</h2>
        <p>
            We thank Patricia Pauli for helpful discussions. This work is funded by the Munich Center for Machine Learning. 
            Julian McGinnis and Mark Mühlau are supported by Bavarian State Ministry for Science and Art (Collaborative Bilateral Research Program Bavaria – Quebec: AI in medicine, grant F.4-V0134.K5.1/86/34). 
            Suprosanna Shit is supported by the UZH Postdoc Grant (K-74851-03-01). 
            Suprosanna Shit and Björn Menze acknowledge support by the Helmut Horten Foundation.
        </p>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <a class="icon-link" href="#">
                <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="https://anonymous.4open.science/r/iclr_lip-88B1/" class="external-link" disabled>
                <i class="fab fa-github"></i>
            </a>
        </div>

        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        This webpage is adapted from the <a
                            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> template.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>

</html>
